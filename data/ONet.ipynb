{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8ee645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6638de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "onet_db_path = os.path.join(base_path, \"ONet\")\n",
    "output_path = os.path.join(base_path, \"ONet_Output\")\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"\n",
    "    Removes special characters from a string to make it a valid filename.\n",
    "    \"\"\"\n",
    "    # Remove characters that are not letters, numbers, underscores, or hyphens\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name)\n",
    "    # Replace whitespace and hyphens with a single underscore\n",
    "    name = re.sub(r'[-\\s]+', '_', name)\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abe3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_onet_data_basic(onet_db_path, output_path):\n",
    "    \"\"\"\n",
    "    Reads O*NET data from Excel files, processes it, and generates a .txt file\n",
    "    for each occupation with its description and tasks.\n",
    "\n",
    "    Args:\n",
    "        onet_db_path (str): The path to the directory containing O*NET .xlsx files.\n",
    "        output_path (str): The directory where the .txt files will be saved.\n",
    "    \"\"\"\n",
    "    print(\"Starting O*NET data processing...\")\n",
    "\n",
    "    # --- 1. Create Output Directory ---\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        print(f\"Created output directory: {output_path}\")\n",
    "\n",
    "    # --- 2. Define File Paths ---\n",
    "    # These are the key files we need for descriptions and tasks.\n",
    "    try:\n",
    "        occupation_data_file = os.path.join(onet_db_path, \"Occupation Data.xlsx\")\n",
    "        tasks_file = os.path.join(onet_db_path, \"Task Statements.xlsx\")\n",
    "\n",
    "        # --- 3. Load Data into Pandas DataFrames ---\n",
    "        print(\"Loading data from Excel files...\")\n",
    "        df_occupations = pd.read_excel(occupation_data_file)\n",
    "        df_tasks = pd.read_excel(tasks_file)\n",
    "        print(\"Data loaded successfully.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Please ensure the O*NET database files are in the specified directory.\")\n",
    "        return\n",
    "\n",
    "    # --- 4. Process Each Occupation ---\n",
    "    print(f\"Found {len(df_occupations)} occupations. Generating files...\")\n",
    "\n",
    "    # Iterate over each row in the occupations DataFrame. Each row is one job.\n",
    "    for index, row in df_occupations.iterrows():\n",
    "        # Get the unique code and title for the current occupation\n",
    "        soc_code = row['O*NET-SOC Code']\n",
    "        title = row['Title']\n",
    "        description = row['Description']\n",
    "\n",
    "        # --- 5. Find All Tasks for the Current Occupation ---\n",
    "        # Filter the tasks DataFrame to get only the tasks for the current soc_code\n",
    "        occupation_tasks = df_tasks[df_tasks['O*NET-SOC Code'] == soc_code]\n",
    "\n",
    "        # --- 6. Generate the .txt File Content ---\n",
    "        # We use f-strings to build the content for our file.\n",
    "        # This is where you will add more data categories later.\n",
    "        file_content = []\n",
    "        file_content.append(f\"Job Title: {title}\")\n",
    "        file_content.append(f\"O*NET-SOC Code: {soc_code}\\n\")\n",
    "\n",
    "        file_content.append(\"# DESCRIPTION\")\n",
    "        file_content.append(f\"{description}\\n\")\n",
    "\n",
    "        file_content.append(\"# TASKS\")\n",
    "        if not occupation_tasks.empty:\n",
    "            for task_index, task_row in occupation_tasks.iterrows():\n",
    "                file_content.append(f\"- {task_row['Task']}\")\n",
    "        else:\n",
    "            file_content.append(\"No specific tasks listed for this occupation.\")\n",
    "\n",
    "        # --- 7. Save the .txt File ---\n",
    "        # Sanitize the title to create a safe filename (e.g., \"Software_Developers\")\n",
    "        sanitized_title = sanitize_filename(title)\n",
    "        output_filename = f\"{sanitized_title}_{soc_code}.txt\"\n",
    "        output_filepath = os.path.join(output_path, output_filename)\n",
    "\n",
    "        # Join the content list into a single string with newlines and write to the file\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(file_content))\n",
    "\n",
    "    print(f\"\\nProcessing complete. {len(df_occupations)} files were generated in '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3f4f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting O*NET data processing...\n",
      "Created output directory: /Users/leozhu/Documents/UBC-CIC-Hackathon-2025/data/ONet_Output\n",
      "Loading data from Excel files...\n",
      "Data loaded successfully.\n",
      "Found 1016 occupations. Generating files...\n",
      "\n",
      "Processing complete. 1016 files were generated in '/Users/leozhu/Documents/UBC-CIC-Hackathon-2025/data/ONet_Output'.\n"
     ]
    }
   ],
   "source": [
    "process_onet_data_basic(onet_db_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa4118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_knowledge(onet_db_path, output_path):\n",
    "    \"\"\"\n",
    "    Appends Knowledge data to the existing occupation .txt files.\n",
    "\n",
    "    Args:\n",
    "        onet_db_path (str): The path to the directory containing O*NET .xlsx files.\n",
    "        output_path (str): The directory where the .txt files are located.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting to add Knowledge data...\")\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        occupation_data_file = os.path.join(onet_db_path, \"Occupation Data.xlsx\")\n",
    "        knowledge_file = os.path.join(onet_db_path, \"Knowledge.xlsx\")\n",
    "\n",
    "        print(\"Loading knowledge data...\")\n",
    "        df_occupations = pd.read_excel(occupation_data_file)\n",
    "        df_knowledge = pd.read_excel(knowledge_file)\n",
    "        print(\"Data loaded.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Could not find a required Excel file.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Process Each Occupation ---\n",
    "    print(f\"Enriching {len(df_occupations)} files with knowledge data...\")\n",
    "    for index, row in df_occupations.iterrows():\n",
    "        soc_code = row['O*NET-SOC Code']\n",
    "        title = row['Title']\n",
    "\n",
    "        # Recreate the filename\n",
    "        sanitized_title = sanitize_filename(title)\n",
    "        output_filename = f\"{sanitized_title}_{soc_code}.txt\"\n",
    "        output_filepath = os.path.join(output_path, output_filename)\n",
    "\n",
    "        if not os.path.exists(output_filepath):\n",
    "            continue\n",
    "\n",
    "        # --- 3. Filter and Format Knowledge Data ---\n",
    "        job_knowledge = (\n",
    "            df_knowledge[df_knowledge['O*NET-SOC Code'] == soc_code]\n",
    "            .pivot_table(\n",
    "                index=['Element Name'],\n",
    "                columns='Scale Name',\n",
    "                values='Data Value'\n",
    "            )\n",
    "            .reset_index()\n",
    "            .fillna(0)\n",
    "        )\n",
    "        knowledge_content = [\"\\n\\n# KNOWLEDGE\"]\n",
    "        # Add context for the LLM\n",
    "        knowledge_content.append(\"(Scale definitions: Importance is rated 1-5. Level is rated 0-7.)\")\n",
    "\n",
    "        if not job_knowledge.empty:\n",
    "            # Sort by importance\n",
    "            job_knowledge = job_knowledge.sort_values(by='Importance', ascending=False)\n",
    "            for _, knowledge_row in job_knowledge.iterrows():\n",
    "                name = knowledge_row['Element Name']\n",
    "                importance = knowledge_row['Importance']\n",
    "                level = knowledge_row['Level']\n",
    "                knowledge_content.append(f\"- {name} (Importance: {importance}/5 | Level: {level}/7)\")\n",
    "        else:\n",
    "            knowledge_content.append(\"- No specific knowledge areas listed.\")\n",
    "\n",
    "        # --- 4. Append to File ---\n",
    "        with open(output_filepath, 'a', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(knowledge_content))\n",
    "\n",
    "    print(\"Successfully added Knowledge data to all files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55011b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting to add Knowledge data...\n",
      "Loading knowledge data...\n",
      "Data loaded.\n",
      "Enriching 1016 files with knowledge data...\n",
      "Successfully added Knowledge data to all files.\n"
     ]
    }
   ],
   "source": [
    "add_knowledge(onet_db_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38349cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_skills_and_tech(onet_db_path, output_path):\n",
    "    \"\"\"\n",
    "    Appends Skills and Technology Tools data to the existing occupation .txt files.\n",
    "\n",
    "    Args:\n",
    "        onet_db_path (str): The path to the directory containing O*NET .xlsx files.\n",
    "        output_path (str): The directory where the .txt files are located.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting to add Skills and Technology data...\")\n",
    "\n",
    "    # --- 1. Define File Paths and Load Data ---\n",
    "    try:\n",
    "        # We need Occupation Data again to map SOC Codes to the filenames\n",
    "        occupation_data_file = os.path.join(onet_db_path, \"Occupation Data.xlsx\")\n",
    "        skills_file = os.path.join(onet_db_path, \"Skills.xlsx\")\n",
    "        tech_file = os.path.join(onet_db_path, \"Technology Skills.xlsx\")\n",
    "        tool_file = os.path.join(onet_db_path, \"Tools Used.xlsx\")\n",
    "\n",
    "        print(\"Loading additional data...\")\n",
    "        df_occupations = pd.read_excel(occupation_data_file)\n",
    "        df_skills = pd.read_excel(skills_file)\n",
    "        df_tech = pd.read_excel(tech_file)\n",
    "        df_tools = pd.read_excel(tool_file)\n",
    "        print(\"Data loaded.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Could not find a required Excel file.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Process Each Occupation and Append to its File ---\n",
    "    print(f\"Enriching {len(df_occupations)} files...\")\n",
    "    for index, row in df_occupations.iterrows():\n",
    "        soc_code = row['O*NET-SOC Code']\n",
    "        title = row['Title']\n",
    "\n",
    "        # Recreate the exact filename from the first step\n",
    "        sanitized_title = sanitize_filename(title)\n",
    "        output_filename = f\"{sanitized_title}_{soc_code}.txt\"\n",
    "        output_filepath = os.path.join(output_path, output_filename)\n",
    "\n",
    "        if not os.path.exists(output_filepath):\n",
    "            continue # Skip if for some reason the base file doesn't exist\n",
    "\n",
    "        # --- 3. Filter and Format Skills Data ---\n",
    "        # Get all skills for the current job\n",
    "        job_skills = (\n",
    "            df_skills[df_skills['O*NET-SOC Code'] == soc_code]\n",
    "            .pivot_table(\n",
    "                index=['Element Name'],\n",
    "                columns='Scale Name',\n",
    "                values='Data Value'\n",
    "            )\n",
    "            .reset_index()\n",
    "            .fillna(0)\n",
    "        )\n",
    "\n",
    "        skills_content = [\"\\n\\n# SKILLS\", \"(Scale definitions: Importance is rated 1-5. Level is rated 0-7.)\"]\n",
    "\n",
    "        if not job_skills.empty:\n",
    "            # Sort skills by importance to show the most relevant ones first\n",
    "            job_skills = job_skills.sort_values(by='Importance', ascending=False)\n",
    "            for _, skill_row in job_skills.iterrows():\n",
    "                skill_name = skill_row['Element Name']\n",
    "                importance = skill_row['Importance']\n",
    "                level = skill_row['Level']\n",
    "                skills_content.append(f\"- {skill_name} (Importance: {importance}/5 | Level: {level}/7)\")\n",
    "        else:\n",
    "            skills_content.append(\"- No specific skills listed.\")\n",
    "\n",
    "\n",
    "        # --- 4. Filter and Format Technology Data ---\n",
    "        job_tech = df_tech[df_tech['O*NET-SOC Code'] == soc_code]\n",
    "\n",
    "        tech_content = [\"\\n\\n# TECHNOLOGY SKILLS & TOOLS\"]\n",
    "        if not job_tech.empty:\n",
    "            # Group technologies by their category for cleaner output\n",
    "            for _, tech_skill_row in job_tech.iterrows():\n",
    "                tech_skill = tech_skill_row[\"Example\"]\n",
    "                commodity_title = tech_skill_row[\"Commodity Title\"]\n",
    "                tech_content.append(f\"- {tech_skill} (Tech skill type: {commodity_title})\")\n",
    "        else:\n",
    "            tech_content.append(\"- No specific technology listed.\")\n",
    "\n",
    "        tool_content = df_tools[df_tools['O*NET-SOC Code'] == soc_code]\n",
    "        if not tool_content.empty:\n",
    "            tech_content.append(\"\\n## TOOLS USED\")\n",
    "            for _, tool_row in tool_content.iterrows():\n",
    "                tool_name = tool_row['Example']\n",
    "                commodity_title = tool_row['Commodity Title']\n",
    "                tech_content.append(f\"- {tool_name} (Tool type: {commodity_title})\")\n",
    "        else:\n",
    "            tech_content.append(\"- No specific tools listed.\")\n",
    "\n",
    "\n",
    "        # --- 5. Append All New Content to the Existing File ---\n",
    "        # We open the file in 'a' (append) mode\n",
    "        with open(output_filepath, 'a', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(skills_content))\n",
    "            f.write('\\n'.join(tech_content))\n",
    "\n",
    "    print(\"Successfully added Skills and Technology data to all files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8488b525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting to add Skills and Technology data...\n",
      "Loading additional data...\n",
      "Data loaded.\n",
      "Enriching 1016 files...\n",
      "Successfully added Skills and Technology data to all files.\n"
     ]
    }
   ],
   "source": [
    "add_skills_and_tech(onet_db_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c92370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context_and_interests(onet_db_path, output_path):\n",
    "    \"\"\"\n",
    "    Appends Work Context and Interests data to the existing occupation .txt files.\n",
    "    This function handles the complexity of merging context categories.\n",
    "\n",
    "    Args:\n",
    "        onet_db_path (str): The path to the directory containing O*NET .xlsx files.\n",
    "        output_path (str): The directory where the .txt files are located.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting to add Work Context and Interests data...\")\n",
    "\n",
    "    # --- 1. Define File Paths and Load Data ---\n",
    "    try:\n",
    "        occupation_data_file = os.path.join(onet_db_path, \"Occupation Data.xlsx\")\n",
    "        work_context_file = os.path.join(onet_db_path, \"Work Context.xlsx\")\n",
    "        context_categories_file = os.path.join(onet_db_path, \"Work Context Categories.xlsx\")\n",
    "        interests_file = os.path.join(onet_db_path, \"Interests.xlsx\")\n",
    "\n",
    "        print(\"Loading context and interests data...\")\n",
    "        df_occupations = pd.read_excel(occupation_data_file)\n",
    "        df_work_context = pd.read_excel(work_context_file)\n",
    "        df_context_categories = pd.read_excel(context_categories_file)\n",
    "        df_interests = pd.read_excel(interests_file)\n",
    "        print(\"Data loaded.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Could not find a required Excel file.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Pre-process and Merge Work Context Data ---\n",
    "    # We only care about the categorical context data, which has the 'CXP' Scale ID.\n",
    "    df_context_categorical = df_work_context[df_work_context['Scale ID'] == 'CXP'].copy()\n",
    "\n",
    "    # Merge with the category descriptions to get human-readable text\n",
    "    # We need to rename the category column in the lookup table to avoid conflicts\n",
    "    df_merged_context = pd.merge(\n",
    "        df_context_categorical,\n",
    "        df_context_categories.rename(columns={'Category': 'Category Code'}),\n",
    "        how='left',\n",
    "        left_on=['Element ID', 'Scale ID', 'Category'],\n",
    "        right_on=['Element ID', 'Scale ID', 'Category Code']\n",
    "    )\n",
    "\n",
    "    # --- 3. Process Each Occupation and Append to its File ---\n",
    "    print(f\"Enriching {len(df_occupations)} files with new data...\")\n",
    "    for index, row in df_occupations.iterrows():\n",
    "        soc_code = row['O*NET-SOC Code']\n",
    "        title = row['Title']\n",
    "\n",
    "        # Recreate the exact filename\n",
    "        sanitized_title = sanitize_filename(title)\n",
    "        output_filename = f\"{sanitized_title}_{soc_code}.txt\"\n",
    "        output_filepath = os.path.join(output_path, output_filename)\n",
    "\n",
    "        if not os.path.exists(output_filepath):\n",
    "            continue\n",
    "\n",
    "        # --- 4. Determine the Most Relevant Work Context for the Job ---\n",
    "        job_context = df_merged_context[df_merged_context['O*NET-SOC Code'] == soc_code]\n",
    "        context_content = [\"\\n\\n# WORK CONTEXT\"]\n",
    "\n",
    "        if not job_context.empty:\n",
    "            # Group by each specific context element (e.g., \"Public Speaking\")\n",
    "            for element_name, group in job_context.groupby('Element Name_x'):\n",
    "                # Find the category with the highest percentage score ('Data Value')\n",
    "                most_relevant = group.loc[group['Data Value'].idxmax()]\n",
    "                description = most_relevant['Category Description']\n",
    "                context_content.append(f\"- {element_name}: {description}\")\n",
    "        else:\n",
    "            context_content.append(\"- No specific work context listed.\")\n",
    "\n",
    "        # --- 5. Determine the Top Interests for the Job ---\n",
    "        job_interests = df_interests[df_interests['O*NET-SOC Code'] == soc_code]\n",
    "        interests_content = [\"\\n\\n# INTERESTS (RIASEC)\"]\n",
    "\n",
    "        if not job_interests.empty:\n",
    "            # Map numbers to RIASEC letters/names\n",
    "            riasec_map = {\n",
    "                1: \"Realistic\",\n",
    "                2: \"Investigative\",\n",
    "                3: \"Artistic\",\n",
    "                4: \"Social\",\n",
    "                5: \"Enterprising\",\n",
    "                6: \"Conventional\"\n",
    "            }\n",
    "\n",
    "            # Get First, Second, Third Interest High-Points if they exist\n",
    "            first_hp = job_interests[job_interests['Element ID'] == '1.B.1.g']\n",
    "            second_hp = job_interests[job_interests['Element ID'] == '1.B.1.h']\n",
    "            third_hp = job_interests[job_interests['Element ID'] == '1.B.1.i']\n",
    "\n",
    "            labels = [\"Primary Interest\", \"Secondary Interest\", \"Tertiary Interest\"]\n",
    "            high_points = [first_hp, second_hp, third_hp]\n",
    "\n",
    "            for label, hp_df in zip(labels, high_points):\n",
    "                if not hp_df.empty:\n",
    "                    interest_num = int(hp_df.iloc[0]['Data Value'])\n",
    "                    interest_name = riasec_map.get(interest_num, f\"Unknown ({interest_num})\")\n",
    "                    interests_content.append(f\"- {label}: {interest_name}\")\n",
    "                else:\n",
    "                    interests_content.append(f\"- {label}: Not available\")\n",
    "        else:\n",
    "            interests_content.append(\"- No specific interests listed.\")\n",
    "\n",
    "        # --- 6. Append All New Content to the Existing File ---\n",
    "        with open(output_filepath, 'a', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(context_content))\n",
    "            f.write('\\n'.join(interests_content))\n",
    "\n",
    "    print(\"Successfully added Work Context and Interests data to all files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4604724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting to add Work Context and Interests data...\n",
      "Loading context and interests data...\n",
      "Data loaded.\n",
      "Enriching 1016 files with new data...\n",
      "Successfully added Work Context and Interests data to all files.\n"
     ]
    }
   ],
   "source": [
    "add_context_and_interests(onet_db_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d55164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_related_occupations(onet_db_path, output_path):\n",
    "    \"\"\"\n",
    "    Appends Related Occupations data to the existing occupation .txt files.\n",
    "\n",
    "    Args:\n",
    "        onet_db_path (str): The path to the directory containing O*NET .xlsx files.\n",
    "        output_path (str): The directory where the .txt files are located.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting to add Related Occupations data...\")\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    try:\n",
    "        occupation_data_file = os.path.join(onet_db_path, \"Occupation Data.xlsx\")\n",
    "        related_occ_file = os.path.join(onet_db_path, \"Related Occupations.xlsx\")\n",
    "\n",
    "        print(\"Loading related occupations data...\")\n",
    "        df_occupations = pd.read_excel(occupation_data_file)\n",
    "        df_related = pd.read_excel(related_occ_file)\n",
    "        print(\"Data loaded.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Could not find a required Excel file.\")\n",
    "        return\n",
    "        \n",
    "    # Create a quick lookup dictionary (hash map) for SOC Code -> Title\n",
    "    # This is much faster than searching the DataFrame in every loop iteration\n",
    "    soc_to_title_map = pd.Series(df_occupations.Title.values, index=df_occupations['O*NET-SOC Code']).to_dict()\n",
    "\n",
    "    # --- 2. Process Each Occupation ---\n",
    "    print(f\"Enriching {len(df_occupations)} files with related occupations...\")\n",
    "    for index, row in df_occupations.iterrows():\n",
    "        soc_code = row['O*NET-SOC Code']\n",
    "        title = row['Title']\n",
    "\n",
    "        # Recreate the filename\n",
    "        sanitized_title = sanitize_filename(title)\n",
    "        output_filename = f\"{sanitized_title}_{soc_code}.txt\"\n",
    "        output_filepath = os.path.join(output_path, output_filename)\n",
    "\n",
    "        if not os.path.exists(output_filepath):\n",
    "            continue\n",
    "\n",
    "        # --- 3. Filter and Format Related Occupations Data ---\n",
    "        job_related_occs = df_related[df_related['O*NET-SOC Code'] == soc_code]\n",
    "        related_content = [\"\\n\\n# RELATED OCCUPATIONS\"]\n",
    "\n",
    "        if not job_related_occs.empty:\n",
    "            for _, related_row in job_related_occs.iterrows():\n",
    "                related_soc = related_row['Related O*NET-SOC Code']\n",
    "                # Use the fast lookup map to get the title\n",
    "                related_title = soc_to_title_map.get(related_soc, \"Unknown Title\")\n",
    "                related_content.append(f\"- {related_title}\")\n",
    "        else:\n",
    "            related_content.append(\"- No related occupations listed.\")\n",
    "\n",
    "        # --- 4. Append to File ---\n",
    "        with open(output_filepath, 'a', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(related_content))\n",
    "\n",
    "    print(\"Successfully added Related Occupations data to all files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85515c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting to add Related Occupations data...\n",
      "Loading related occupations data...\n",
      "Data loaded.\n",
      "Enriching 1016 files with related occupations...\n",
      "Successfully added Related Occupations data to all files.\n"
     ]
    }
   ],
   "source": [
    "add_related_occupations(onet_db_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16130c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
